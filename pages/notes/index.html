<!DOCTYPE html>
<html>
<head>
	<meta charset="utf-8" />
	<meta http-equiv="X-UA-Compatible" content="IE=edge"><title>Academic Notes - Christoph Heindl</title><link rel="apple-touch-icon" sizes="180x180" href="https://cheind.github.io/apple-touch-icon.png">
	<link rel="icon" type="image/png" sizes="32x32" href="https://cheind.github.io/favicon-32x32.png">
	<link rel="icon" type="image/png" sizes="16x16" href="https://cheind.github.io/favicon-16x16.png">
	<link rel="manifest" href="https://cheind.github.io/site.webmanifest">
	<link rel="mask-icon" href="https://cheind.github.io/safari-pinned-tab.svg" color="#5bbad5">
	<meta name="msapplication-TileColor" content="#da532c">
	<meta name="theme-color" content="#ffffff">

	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta property="og:title" content="Academic Notes" />
<meta property="og:description" content="Here is a collection of notes I have prepared in the past on various topics of interest.
 Semi-supervised Expectation Maximization This work considers the Expectation Maximization (EM) algorithm in the semi-supervised setting. First, the general form for semi-supervised version of maximum likelihood is derived from the Latent Variable Model (LVM). Since the involved integrals are usually intractable, a surrogate objective function based on the Evidence Lower Bound (ELBO) is introduced." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cheind.github.io/pages/notes/" />
<meta property="article:published_time" content="2021-02-03T14:39:22+01:00" />
<meta property="article:modified_time" content="2021-02-03T14:39:22+01:00" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Academic Notes"/>
<meta name="twitter:description" content="Here is a collection of notes I have prepared in the past on various topics of interest.
 Semi-supervised Expectation Maximization This work considers the Expectation Maximization (EM) algorithm in the semi-supervised setting. First, the general form for semi-supervised version of maximum likelihood is derived from the Latent Variable Model (LVM). Since the involved integrals are usually intractable, a surrogate objective function based on the Evidence Lower Bound (ELBO) is introduced."/>
<link href="https://fonts.googleapis.com/css?family=Ubuntu:300,400,300italic,400italic|Raleway:500,100,300" rel="stylesheet">

	<link rel="stylesheet" type="text/css" media="screen" href="https://cheind.github.io/css/normalize.css" />
	<link rel="stylesheet" type="text/css" media="screen" href="https://cheind.github.io/css/main.css"/>

	<script src="https://cheind.github.io/js/feather.min.js"></script><script src="https://cheind.github.io/js/main.js"></script>
</head>


<body>
    <div id="content">





<div class="header wrapper">
  <span class="page_title">
    Academic Notes
  </span>
  <ul class="flat">
    <li class="page_meta">February 3, 2021</li>
  </ul>
  <a href="https://cheind.github.io" title="Home" class="page_meta">[home]</a>
</div>

<div class="content wrapper">
  <p>Here is a collection of notes I have prepared in the past on various topics of interest.</p>
<!-- raw HTML omitted -->
<h4 id="hahahugoshortcode-s0-hbhb-semi-supervised-expectation-maximization"><i class="feather-16" data-feather="corner-down-right"></i> Semi-supervised Expectation Maximization</h4>
<p>This work considers the Expectation Maximization (EM) algorithm in the semi-supervised setting. First, the general form for semi-supervised version of maximum likelihood is derived from the Latent Variable Model (LVM). Since the involved integrals are usually intractable, a surrogate objective function based on the Evidence Lower Bound (ELBO) is introduced. Next, we derive the equations of the semi-supervised EM. Finally, the concrete equations for a fitting a Gaussian Mixture Model (GMM) using labeled and unlabeled data are deduced.<br>
<a href="/files/Notes_on_Semi_Supervised_Expectation_Maximization.pdf">[pdf]</a>
<a href="/files/EM.ipynb">[ipynb]</a></p>
<h4 id="hahahugoshortcode-s1-hbhb-variational-autoencoders"><i class="feather-16" data-feather="corner-down-right"></i> Variational Autoencoders</h4>
<p>We seek to perform density estimation between a true (but unknown) probability distribution, and a set of proposal distributions.  Formulated as minimization between distributions, this leads to the maximum (marginal) likelihood principle (see Section 2). To extend the expressiveness of our proposal distributions, we introduce latent variable models (see Section 3).  The hidden variables raise issues in evaluating the marginal likelihood.  We replace the marginal likelihood by a surrogate objective developed by Variational Inference (see Section 4).  Combining these ideas gives us a powerful framework to estimate marginal, inference and joint distributions simultaneously. The Variational Autoencoder(see Section 5) is a specific application of these framework, that parametrizes the distributions to be optimized by neural networks.<br>
<a href="/files/Notes_on_Variational_Autoencoders.pdf">[pdf]</a>
<a href="/files/vae.zip">[ipynb]</a></p>
<h4 id="hahahugoshortcode-s2-hbhb-stochastic-optimization-of-non-differentiable-functions"><i class="feather-16" data-feather="corner-down-right"></i> Stochastic Optimization of Non-Differentiable Functions</h4>
<p>We consider a gradient based parameter optimization of a stochastic computational graph consisting of random scene properties and a deterministic, non-differentiable render function. Our approach leverages ideas from Generative Adverserial Networks (GANs) and gradient estimators from reinforcement learning to jointly optimize all distributional and structural parameters based on generated images. This document should be regarded as an unfinished notebook to emphasize our main idea.<br>
<a href="/files/Notes_on_Stochastic_Optimization_for_BlendTorch.pdf">[pdf]</a>
<a href="https://github.com/cheind/pytorch-blender/tree/develop/examples/densityopt">[blendtorch]</a></p>
<h4 id="hahahugoshortcode-s3-hbhb-graph-neural-networks-for-node-level-predictions"><i class="feather-16" data-feather="corner-down-right"></i> Graph Neural Networks for Node-Level Predictions</h4>
<p>This work aims to provide an overview of early and modern graph neural network based machine learning methods for node-level prediction tasks. Under the umbrella of taxonomies already established in the literature, we explain the core concepts and provide detailed explanations for convolutional methods that have had strong impact. In addition, we introduce common benchmarks and present selected applications from various areas. Finally, we discuss open problems for further research.<br>
<a href="/files/2007.08649.pdf">[pdf]</a>
<a href="https://arxiv.org/abs/2007.08649">[arXiv]</a></p>

</div>

</div>
    <div class="footer wrapper">
	<nav class="nav">
		<div>Christoph Heindl 2021</div>
	</nav>
</div>


<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-176158287-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>
<script>
	feather.replace()
</script>
</body>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>

</html>